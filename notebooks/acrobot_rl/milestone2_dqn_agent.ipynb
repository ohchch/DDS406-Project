{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# milestone2_dqn.py\n",
    "\n",
    "# --- 0. 导入必要的库和设置随机种子 ---\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import gym # 根据您提供的 requirements.txt，直接导入 gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Keras-RL2 imports\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Callback\n",
    "from rl.processors import Processor # <--- 新增导入\n",
    "\n",
    "# 设置随机种子以保证结果可复现性\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境名称: Acrobot-v1\n",
      "观测空间形状: (6,)\n",
      "动作空间大小: 3\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 环境初始化 ---\n",
    "ENV_NAME = 'Acrobot-v1'\n",
    "env = gym.make(ENV_NAME)\n",
    "_ = env.reset(seed=SEED)\n",
    "\n",
    "# 添加环境包装以兼容 keras-rl2 的 step() 方法\n",
    "class KerasRL2Wrapper(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        # 新版 gym 的 step 返回 (observation, reward, terminated, truncated, info)\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # keras-rl2 需要 (observation, reward, done, info)\n",
    "        # done 为 True 如果 terminated 或 truncated 为 True\n",
    "        done = terminated or truncated\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    # reset 方法在新的 Gym 版本中返回 (observation, info)\n",
    "    # 您的 CustomProcessor 已经处理了 reset 返回元组的情况，所以这里不需要修改 reset 方法\n",
    "\n",
    "# 应用包装\n",
    "env = KerasRL2Wrapper(env)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "input_shape = env.observation_space.shape\n",
    "\n",
    "print(f\"环境名称: {ENV_NAME}\")\n",
    "print(f\"观测空间形状: {input_shape}\")\n",
    "print(f\"动作空间大小: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 构建的 Q-网络模型摘要 ---\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4803 (18.76 KB)\n",
      "Trainable params: 4803 (18.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 设计神经网络架构 (Q-网络) ---\n",
    "def build_model(input_shape, nb_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + input_shape))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape, nb_actions)\n",
    "print(\"\\n--- 构建的 Q-网络模型摘要 ---\")\n",
    "model.summary()\n",
    "\n",
    "# --- 定義一個簡單的 Processor 類別 ---\n",
    "class CustomProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # 確保觀測值是 float32 類型，這是 TensorFlow 常見的輸入類型\n",
    "        # 檢查是否為元組，如果是，取第一個元素\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]\n",
    "        return observation.astype('float32')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # 確保批處理的形狀適合模型輸入\n",
    "        # 對於 window_length=1，batch 已經會是 (N, 1, *obs_shape)\n",
    "        # Flatten 層會將其處理為 (N, flat_obs_size)\n",
    "        return batch.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 构建 DQN 代理 (里程碑2的第1步) ---\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, nb_steps=50000, value_test=.05)\n",
    "\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=1000,\n",
    "    gamma=.99,\n",
    "    target_model_update=10000,\n",
    "    train_interval=4,\n",
    "    processor=CustomProcessor() # <--- 在这里添加处理器\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:39:49.766958: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_17/bias/Assign' id:1339 op device:{requested: '', assigned: ''} def:{{{node dense_17/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_17/bias, dense_17/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DQN 代理编译完成 ---\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 编译代理 (里程碑2的第2步) ---\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "print(\"\\n--- DQN 代理编译完成 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始训练 DQN 代理，共 50000 步 ---\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:39:50.886456: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_17/BiasAdd' id:1344 op device:{requested: '', assigned: ''} def:{{{node dense_17/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_17/MatMul, dense_17/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-06-10 14:39:51.106977: W tensorflow/c/c_api.cc:305] Operation '{name:'total_12/Assign' id:1500 op device:{requested: '', assigned: ''} def:{{{node total_12/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_12, total_12/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500/50000: episode: 1, duration: 4.470s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1000/50000: episode: 2, duration: 3.947s, episode steps: 500, steps per second: 127, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2025-06-10 14:39:59.308357: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_17_1/BiasAdd' id:1418 op device:{requested: '', assigned: ''} def:{{{node dense_17_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_17_1/MatMul, dense_17_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-06-10 14:40:00.104449: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_15/AddN' id:1631 op device:{requested: '', assigned: ''} def:{{{node loss_15/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul, loss_15/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-06-10 14:40:00.298918: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/beta_2/Assign' id:1762 op device:{requested: '', assigned: ''} def:{{{node training/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/beta_2, training/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1500/50000: episode: 3, duration: 11.180s, episode steps: 500, steps per second:  45, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.052684, mae: 0.392730, mean_q: -0.360380, mean_eps: 0.977500\n",
      "  2000/50000: episode: 4, duration: 9.051s, episode steps: 500, steps per second:  55, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.003845, mae: 0.398548, mean_q: -0.487247, mean_eps: 0.968536\n",
      "  2500/50000: episode: 5, duration: 7.396s, episode steps: 500, steps per second:  68, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.002944, mae: 0.388470, mean_q: -0.469175, mean_eps: 0.959536\n",
      "  3000/50000: episode: 6, duration: 3.452s, episode steps: 500, steps per second: 145, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.002559, mae: 0.376311, mean_q: -0.440944, mean_eps: 0.950536\n",
      "  3500/50000: episode: 7, duration: 1.608s, episode steps: 500, steps per second: 311, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.002556, mae: 0.384361, mean_q: -0.450053, mean_eps: 0.941536\n",
      "  4000/50000: episode: 8, duration: 1.608s, episode steps: 500, steps per second: 311, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.002030, mae: 0.380329, mean_q: -0.452717, mean_eps: 0.932536\n",
      "  4500/50000: episode: 9, duration: 1.440s, episode steps: 500, steps per second: 347, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.002304, mae: 0.380800, mean_q: -0.453334, mean_eps: 0.923536\n",
      "  5000/50000: episode: 10, duration: 1.379s, episode steps: 500, steps per second: 363, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.002968, mae: 0.380698, mean_q: -0.441137, mean_eps: 0.914536\n",
      "  5500/50000: episode: 11, duration: 1.427s, episode steps: 500, steps per second: 350, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.002298, mae: 0.378738, mean_q: -0.446759, mean_eps: 0.905536\n",
      "  6000/50000: episode: 12, duration: 1.400s, episode steps: 500, steps per second: 357, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.001932, mae: 0.375384, mean_q: -0.441271, mean_eps: 0.896536\n",
      "  6441/50000: episode: 13, duration: 1.301s, episode steps: 441, steps per second: 339, episode reward: -440.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.003022, mae: 0.387917, mean_q: -0.459706, mean_eps: 0.888040\n",
      "  6815/50000: episode: 14, duration: 1.142s, episode steps: 374, steps per second: 328, episode reward: -373.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.002066, mae: 0.383661, mean_q: -0.451381, mean_eps: 0.880696\n",
      "  7315/50000: episode: 15, duration: 1.554s, episode steps: 500, steps per second: 322, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.002559, mae: 0.382062, mean_q: -0.450086, mean_eps: 0.872848\n",
      "  7584/50000: episode: 16, duration: 1.055s, episode steps: 269, steps per second: 255, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.001834, mae: 0.373325, mean_q: -0.432420, mean_eps: 0.865936\n",
      "  8084/50000: episode: 17, duration: 1.352s, episode steps: 500, steps per second: 370, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.001475, mae: 0.378029, mean_q: -0.454309, mean_eps: 0.859024\n",
      "  8524/50000: episode: 18, duration: 1.347s, episode steps: 440, steps per second: 327, episode reward: -439.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.001439, mae: 0.373279, mean_q: -0.443239, mean_eps: 0.850564\n",
      "  9024/50000: episode: 19, duration: 2.522s, episode steps: 500, steps per second: 198, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.002073, mae: 0.371290, mean_q: -0.436234, mean_eps: 0.842104\n",
      "  9376/50000: episode: 20, duration: 2.284s, episode steps: 352, steps per second: 154, episode reward: -351.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.001877, mae: 0.371243, mean_q: -0.423335, mean_eps: 0.834436\n",
      "  9870/50000: episode: 21, duration: 3.606s, episode steps: 494, steps per second: 137, episode reward: -493.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.002339, mae: 0.367843, mean_q: -0.413948, mean_eps: 0.826804\n",
      " 10370/50000: episode: 22, duration: 7.265s, episode steps: 500, steps per second:  69, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.027746, mae: 0.821869, mean_q: -1.092986, mean_eps: 0.817840\n",
      " 10870/50000: episode: 23, duration: 8.031s, episode steps: 500, steps per second:  62, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.004890, mae: 0.969909, mean_q: -1.375165, mean_eps: 0.808840\n",
      " 11370/50000: episode: 24, duration: 6.929s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.003645, mae: 0.963446, mean_q: -1.370244, mean_eps: 0.799840\n",
      " 11851/50000: episode: 25, duration: 6.420s, episode steps: 481, steps per second:  75, episode reward: -480.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.002538, mae: 0.970129, mean_q: -1.388189, mean_eps: 0.791020\n",
      " 12235/50000: episode: 26, duration: 5.800s, episode steps: 384, steps per second:  66, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.002276, mae: 0.963772, mean_q: -1.378712, mean_eps: 0.783244\n",
      " 12708/50000: episode: 27, duration: 7.475s, episode steps: 473, steps per second:  63, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.002084, mae: 0.973773, mean_q: -1.394883, mean_eps: 0.775540\n",
      " 13065/50000: episode: 28, duration: 4.846s, episode steps: 357, steps per second:  74, episode reward: -356.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.001573, mae: 0.971900, mean_q: -1.394694, mean_eps: 0.768052\n",
      " 13483/50000: episode: 29, duration: 4.819s, episode steps: 418, steps per second:  87, episode reward: -417.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.002 [0.000, 2.000],  loss: 0.001324, mae: 0.959436, mean_q: -1.376658, mean_eps: 0.761068\n",
      " 13817/50000: episode: 30, duration: 4.142s, episode steps: 334, steps per second:  81, episode reward: -333.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.832 [0.000, 2.000],  loss: 0.002063, mae: 0.960080, mean_q: -1.377839, mean_eps: 0.754300\n",
      " 14200/50000: episode: 31, duration: 5.244s, episode steps: 383, steps per second:  73, episode reward: -382.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.001696, mae: 0.959759, mean_q: -1.378049, mean_eps: 0.747856\n",
      " 14484/50000: episode: 32, duration: 3.894s, episode steps: 284, steps per second:  73, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.001968, mae: 0.962067, mean_q: -1.383373, mean_eps: 0.741880\n",
      " 14879/50000: episode: 33, duration: 5.568s, episode steps: 395, steps per second:  71, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.002165, mae: 0.962171, mean_q: -1.380193, mean_eps: 0.735760\n",
      " 15178/50000: episode: 34, duration: 3.682s, episode steps: 299, steps per second:  81, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.002522, mae: 0.962544, mean_q: -1.377601, mean_eps: 0.729496\n",
      " 15480/50000: episode: 35, duration: 2.082s, episode steps: 302, steps per second: 145, episode reward: -301.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.002985, mae: 0.954016, mean_q: -1.367126, mean_eps: 0.724096\n",
      " 15815/50000: episode: 36, duration: 0.976s, episode steps: 335, steps per second: 343, episode reward: -334.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.001949, mae: 0.956493, mean_q: -1.371632, mean_eps: 0.718372\n",
      " 16082/50000: episode: 37, duration: 1.031s, episode steps: 267, steps per second: 259, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.001275, mae: 0.962394, mean_q: -1.384592, mean_eps: 0.712936\n",
      " 16384/50000: episode: 38, duration: 1.068s, episode steps: 302, steps per second: 283, episode reward: -301.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.002105, mae: 0.957017, mean_q: -1.373817, mean_eps: 0.707824\n",
      " 16729/50000: episode: 39, duration: 1.629s, episode steps: 345, steps per second: 212, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.002192, mae: 0.955596, mean_q: -1.372437, mean_eps: 0.701992\n",
      " 16994/50000: episode: 40, duration: 0.991s, episode steps: 265, steps per second: 268, episode reward: -264.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.002691, mae: 0.954439, mean_q: -1.368558, mean_eps: 0.696484\n",
      " 17288/50000: episode: 41, duration: 1.459s, episode steps: 294, steps per second: 201, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.002097, mae: 0.958114, mean_q: -1.372528, mean_eps: 0.691480\n",
      " 17636/50000: episode: 42, duration: 1.668s, episode steps: 348, steps per second: 209, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.002219, mae: 0.950409, mean_q: -1.362453, mean_eps: 0.685720\n",
      " 17930/50000: episode: 43, duration: 1.594s, episode steps: 294, steps per second: 184, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.003042, mae: 0.956306, mean_q: -1.368832, mean_eps: 0.679924\n",
      " 18356/50000: episode: 44, duration: 2.972s, episode steps: 426, steps per second: 143, episode reward: -425.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.001758, mae: 0.952255, mean_q: -1.365166, mean_eps: 0.673444\n",
      " 18587/50000: episode: 45, duration: 1.027s, episode steps: 231, steps per second: 225, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.001235, mae: 0.957719, mean_q: -1.374315, mean_eps: 0.667540\n",
      " 18859/50000: episode: 46, duration: 0.970s, episode steps: 272, steps per second: 281, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.001344, mae: 0.958521, mean_q: -1.379262, mean_eps: 0.663004\n",
      " 19103/50000: episode: 47, duration: 0.729s, episode steps: 244, steps per second: 335, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.996 [0.000, 2.000],  loss: 0.001603, mae: 0.954900, mean_q: -1.369469, mean_eps: 0.658360\n",
      " 19330/50000: episode: 48, duration: 0.708s, episode steps: 227, steps per second: 321, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.001747, mae: 0.950296, mean_q: -1.368009, mean_eps: 0.654112\n",
      " 19606/50000: episode: 49, duration: 1.139s, episode steps: 276, steps per second: 242, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.002946, mae: 0.949850, mean_q: -1.357603, mean_eps: 0.649576\n",
      " 19949/50000: episode: 50, duration: 1.178s, episode steps: 343, steps per second: 291, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.001312, mae: 0.957336, mean_q: -1.373799, mean_eps: 0.643996\n",
      " 20411/50000: episode: 51, duration: 2.003s, episode steps: 462, steps per second: 231, episode reward: -461.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.038512, mae: 1.540497, mean_q: -2.169966, mean_eps: 0.636760\n",
      " 20696/50000: episode: 52, duration: 1.092s, episode steps: 285, steps per second: 261, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.009419, mae: 1.588421, mean_q: -2.299341, mean_eps: 0.630064\n",
      " 21196/50000: episode: 53, duration: 1.862s, episode steps: 500, steps per second: 268, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.904 [0.000, 2.000],  loss: 0.008613, mae: 1.592875, mean_q: -2.315390, mean_eps: 0.623008\n",
      " 21696/50000: episode: 54, duration: 1.761s, episode steps: 500, steps per second: 284, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.756 [0.000, 2.000],  loss: 0.007002, mae: 1.580779, mean_q: -2.303558, mean_eps: 0.614008\n",
      " 22196/50000: episode: 55, duration: 1.979s, episode steps: 500, steps per second: 253, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.006370, mae: 1.588134, mean_q: -2.318628, mean_eps: 0.605008\n",
      " 22696/50000: episode: 56, duration: 2.195s, episode steps: 500, steps per second: 228, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.006173, mae: 1.582871, mean_q: -2.313911, mean_eps: 0.596008\n",
      " 23196/50000: episode: 57, duration: 1.986s, episode steps: 500, steps per second: 252, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.005182, mae: 1.580734, mean_q: -2.312634, mean_eps: 0.587008\n",
      " 23482/50000: episode: 58, duration: 1.059s, episode steps: 286, steps per second: 270, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.003254, mae: 1.588817, mean_q: -2.325360, mean_eps: 0.579916\n",
      " 23982/50000: episode: 59, duration: 1.684s, episode steps: 500, steps per second: 297, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.894 [0.000, 2.000],  loss: 0.008271, mae: 1.590155, mean_q: -2.322302, mean_eps: 0.572824\n",
      " 24421/50000: episode: 60, duration: 1.477s, episode steps: 439, steps per second: 297, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.004267, mae: 1.587642, mean_q: -2.325170, mean_eps: 0.564364\n",
      " 24921/50000: episode: 61, duration: 1.859s, episode steps: 500, steps per second: 269, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 0.003250, mae: 1.594309, mean_q: -2.339298, mean_eps: 0.555904\n",
      " 25421/50000: episode: 62, duration: 1.631s, episode steps: 500, steps per second: 307, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.848 [0.000, 2.000],  loss: 0.004073, mae: 1.582758, mean_q: -2.317372, mean_eps: 0.546904\n",
      " 25921/50000: episode: 63, duration: 2.520s, episode steps: 500, steps per second: 198, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.736 [0.000, 2.000],  loss: 0.003431, mae: 1.588562, mean_q: -2.331008, mean_eps: 0.537904\n",
      " 26421/50000: episode: 64, duration: 1.483s, episode steps: 500, steps per second: 337, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.896 [0.000, 2.000],  loss: 0.003317, mae: 1.587057, mean_q: -2.328222, mean_eps: 0.528904\n",
      " 26650/50000: episode: 65, duration: 0.679s, episode steps: 229, steps per second: 337, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.006256, mae: 1.585583, mean_q: -2.316273, mean_eps: 0.522352\n",
      " 27150/50000: episode: 66, duration: 1.911s, episode steps: 500, steps per second: 262, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.878 [0.000, 2.000],  loss: 0.006956, mae: 1.593708, mean_q: -2.330445, mean_eps: 0.515800\n",
      " 27443/50000: episode: 67, duration: 1.046s, episode steps: 293, steps per second: 280, episode reward: -292.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.007564, mae: 1.597063, mean_q: -2.333988, mean_eps: 0.508672\n",
      " 27907/50000: episode: 68, duration: 1.474s, episode steps: 464, steps per second: 315, episode reward: -463.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.003546, mae: 1.594493, mean_q: -2.336194, mean_eps: 0.501868\n",
      " 28350/50000: episode: 69, duration: 1.778s, episode steps: 443, steps per second: 249, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.004079, mae: 1.582099, mean_q: -2.320241, mean_eps: 0.493696\n",
      " 28850/50000: episode: 70, duration: 2.323s, episode steps: 500, steps per second: 215, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.836 [0.000, 2.000],  loss: 0.006406, mae: 1.597595, mean_q: -2.336483, mean_eps: 0.485200\n",
      " 29095/50000: episode: 71, duration: 0.843s, episode steps: 245, steps per second: 291, episode reward: -244.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.661 [0.000, 2.000],  loss: 0.002080, mae: 1.600251, mean_q: -2.345890, mean_eps: 0.478504\n",
      " 29595/50000: episode: 72, duration: 1.473s, episode steps: 500, steps per second: 340, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.824 [0.000, 2.000],  loss: 0.005454, mae: 1.600517, mean_q: -2.340810, mean_eps: 0.471808\n",
      " 29879/50000: episode: 73, duration: 0.845s, episode steps: 284, steps per second: 336, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.005694, mae: 1.607938, mean_q: -2.353410, mean_eps: 0.464752\n",
      " 30094/50000: episode: 74, duration: 0.941s, episode steps: 215, steps per second: 229, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.053610, mae: 1.891463, mean_q: -2.693865, mean_eps: 0.460252\n",
      " 30594/50000: episode: 75, duration: 2.289s, episode steps: 500, steps per second: 218, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.778 [0.000, 2.000],  loss: 0.015554, mae: 2.251401, mean_q: -3.280851, mean_eps: 0.453808\n",
      " 31094/50000: episode: 76, duration: 2.830s, episode steps: 500, steps per second: 177, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.762 [0.000, 2.000],  loss: 0.011993, mae: 2.233365, mean_q: -3.281875, mean_eps: 0.444808\n",
      " 31594/50000: episode: 77, duration: 2.708s, episode steps: 500, steps per second: 185, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000],  loss: 0.010487, mae: 2.239583, mean_q: -3.296691, mean_eps: 0.435808\n",
      " 32094/50000: episode: 78, duration: 2.376s, episode steps: 500, steps per second: 210, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.842 [0.000, 2.000],  loss: 0.010172, mae: 2.243059, mean_q: -3.298926, mean_eps: 0.426808\n",
      " 32586/50000: episode: 79, duration: 1.627s, episode steps: 492, steps per second: 302, episode reward: -491.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.005230, mae: 2.253338, mean_q: -3.325055, mean_eps: 0.417880\n",
      " 32911/50000: episode: 80, duration: 1.533s, episode steps: 325, steps per second: 212, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.010557, mae: 2.247272, mean_q: -3.308250, mean_eps: 0.410536\n",
      " 33411/50000: episode: 81, duration: 1.964s, episode steps: 500, steps per second: 255, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.898 [0.000, 2.000],  loss: 0.009570, mae: 2.237581, mean_q: -3.300984, mean_eps: 0.403120\n",
      " 33911/50000: episode: 82, duration: 2.317s, episode steps: 500, steps per second: 216, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.818 [0.000, 2.000],  loss: 0.007522, mae: 2.245351, mean_q: -3.310524, mean_eps: 0.394120\n",
      " 34411/50000: episode: 83, duration: 2.410s, episode steps: 500, steps per second: 207, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.808 [0.000, 2.000],  loss: 0.007426, mae: 2.241595, mean_q: -3.307074, mean_eps: 0.385120\n",
      " 34911/50000: episode: 84, duration: 2.448s, episode steps: 500, steps per second: 204, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 0.009712, mae: 2.250161, mean_q: -3.316891, mean_eps: 0.376120\n",
      " 35411/50000: episode: 85, duration: 1.940s, episode steps: 500, steps per second: 258, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.008938, mae: 2.250477, mean_q: -3.317142, mean_eps: 0.367120\n",
      " 35911/50000: episode: 86, duration: 2.431s, episode steps: 500, steps per second: 206, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.838 [0.000, 2.000],  loss: 0.005259, mae: 2.253317, mean_q: -3.328431, mean_eps: 0.358120\n",
      " 36411/50000: episode: 87, duration: 2.047s, episode steps: 500, steps per second: 244, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.007864, mae: 2.251353, mean_q: -3.326817, mean_eps: 0.349120\n",
      " 36882/50000: episode: 88, duration: 1.742s, episode steps: 471, steps per second: 270, episode reward: -470.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.007581, mae: 2.251509, mean_q: -3.323389, mean_eps: 0.340372\n",
      " 37223/50000: episode: 89, duration: 1.058s, episode steps: 341, steps per second: 322, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.004347, mae: 2.250556, mean_q: -3.328711, mean_eps: 0.333064\n",
      " 37723/50000: episode: 90, duration: 1.367s, episode steps: 500, steps per second: 366, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.902 [0.000, 2.000],  loss: 0.007008, mae: 2.251306, mean_q: -3.325449, mean_eps: 0.325504\n",
      " 38223/50000: episode: 91, duration: 1.377s, episode steps: 500, steps per second: 363, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.007314, mae: 2.250087, mean_q: -3.326237, mean_eps: 0.316504\n",
      " 38723/50000: episode: 92, duration: 1.331s, episode steps: 500, steps per second: 376, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.852 [0.000, 2.000],  loss: 0.006472, mae: 2.252104, mean_q: -3.328972, mean_eps: 0.307504\n",
      " 39223/50000: episode: 93, duration: 1.223s, episode steps: 500, steps per second: 409, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.008768, mae: 2.255169, mean_q: -3.326518, mean_eps: 0.298504\n",
      " 39723/50000: episode: 94, duration: 1.700s, episode steps: 500, steps per second: 294, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.882 [0.000, 2.000],  loss: 0.006967, mae: 2.262952, mean_q: -3.343018, mean_eps: 0.289504\n",
      " 40223/50000: episode: 95, duration: 1.991s, episode steps: 500, steps per second: 251, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.033707, mae: 2.550185, mean_q: -3.721498, mean_eps: 0.280504\n",
      " 40619/50000: episode: 96, duration: 1.393s, episode steps: 396, steps per second: 284, episode reward: -395.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.023071, mae: 2.875909, mean_q: -4.225631, mean_eps: 0.272440\n",
      " 41119/50000: episode: 97, duration: 1.816s, episode steps: 500, steps per second: 275, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.014037, mae: 2.878329, mean_q: -4.247037, mean_eps: 0.264376\n",
      " 41619/50000: episode: 98, duration: 2.411s, episode steps: 500, steps per second: 207, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.019131, mae: 2.873418, mean_q: -4.237352, mean_eps: 0.255376\n",
      " 42119/50000: episode: 99, duration: 2.445s, episode steps: 500, steps per second: 204, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.011648, mae: 2.869974, mean_q: -4.241209, mean_eps: 0.246376\n",
      " 42614/50000: episode: 100, duration: 2.006s, episode steps: 495, steps per second: 247, episode reward: -494.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.089 [0.000, 2.000],  loss: 0.018051, mae: 2.875661, mean_q: -4.242931, mean_eps: 0.237412\n",
      " 43114/50000: episode: 101, duration: 1.870s, episode steps: 500, steps per second: 267, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.184 [0.000, 2.000],  loss: 0.014770, mae: 2.871381, mean_q: -4.239603, mean_eps: 0.228448\n",
      " 43614/50000: episode: 102, duration: 2.403s, episode steps: 500, steps per second: 208, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.015799, mae: 2.868308, mean_q: -4.234745, mean_eps: 0.219448\n",
      " 43835/50000: episode: 103, duration: 1.319s, episode steps: 221, steps per second: 168, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.016037, mae: 2.878878, mean_q: -4.254548, mean_eps: 0.212968\n",
      " 44335/50000: episode: 104, duration: 1.985s, episode steps: 500, steps per second: 252, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.009481, mae: 2.872446, mean_q: -4.250525, mean_eps: 0.206488\n",
      " 44728/50000: episode: 105, duration: 2.054s, episode steps: 393, steps per second: 191, episode reward: -392.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.016007, mae: 2.872697, mean_q: -4.238437, mean_eps: 0.198460\n",
      " 45228/50000: episode: 106, duration: 2.371s, episode steps: 500, steps per second: 211, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.016485, mae: 2.870174, mean_q: -4.232877, mean_eps: 0.190432\n",
      " 45728/50000: episode: 107, duration: 2.820s, episode steps: 500, steps per second: 177, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.019283, mae: 2.877849, mean_q: -4.244348, mean_eps: 0.181432\n",
      " 46228/50000: episode: 108, duration: 4.363s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.878 [0.000, 2.000],  loss: 0.016877, mae: 2.869984, mean_q: -4.238585, mean_eps: 0.172432\n",
      " 46728/50000: episode: 109, duration: 4.340s, episode steps: 500, steps per second: 115, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.022079, mae: 2.872969, mean_q: -4.234234, mean_eps: 0.163432\n",
      " 47228/50000: episode: 110, duration: 5.520s, episode steps: 500, steps per second:  91, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.792 [0.000, 2.000],  loss: 0.012690, mae: 2.872030, mean_q: -4.237265, mean_eps: 0.154432\n",
      " 47728/50000: episode: 111, duration: 5.392s, episode steps: 500, steps per second:  93, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.724 [0.000, 2.000],  loss: 0.015373, mae: 2.876253, mean_q: -4.246803, mean_eps: 0.145432\n",
      " 48228/50000: episode: 112, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.014637, mae: 2.874614, mean_q: -4.241153, mean_eps: 0.136432\n",
      " 48728/50000: episode: 113, duration: 2.368s, episode steps: 500, steps per second: 211, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 0.021528, mae: 2.876116, mean_q: -4.244848, mean_eps: 0.127432\n",
      " 49228/50000: episode: 114, duration: 2.970s, episode steps: 500, steps per second: 168, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.012862, mae: 2.876042, mean_q: -4.254259, mean_eps: 0.118432\n",
      " 49728/50000: episode: 115, duration: 2.726s, episode steps: 500, steps per second: 183, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.010466, mae: 2.875494, mean_q: -4.251841, mean_eps: 0.109432\n",
      "done, took 294.215 seconds\n",
      "\n",
      "--- 训练完成 ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 训练代理 (里程碑2的第3步) ---\n",
    "NB_TRAINING_STEPS = 50000\n",
    "\n",
    "log_filename = 'dqn_acrobot_log.json'\n",
    "weights_filename = 'dqn_acrobot_weights.h5'\n",
    "checkpoint_weights_filename = 'dqn_acrobot_weights_{step}.h5'\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint(checkpoint_weights_filename, interval=10000),\n",
    "    FileLogger(log_filename, interval=100)\n",
    "]\n",
    "\n",
    "print(f\"\\n--- 开始训练 DQN 代理，共 {NB_TRAINING_STEPS} 步 ---\")\n",
    "dqn.fit(env, nb_steps=NB_TRAINING_STEPS, visualize=False, verbose=2, callbacks=callbacks)\n",
    "print(\"\\n--- 训练完成 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始评估训练后的代理，共 10 回合 ---\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -500.000, steps: 500\n",
      "Episode 2: reward: -500.000, steps: 500\n",
      "Episode 3: reward: -500.000, steps: 500\n",
      "Episode 4: reward: -500.000, steps: 500\n",
      "Episode 5: reward: -500.000, steps: 500\n",
      "Episode 6: reward: -500.000, steps: 500\n",
      "Episode 7: reward: -500.000, steps: 500\n",
      "Episode 8: reward: -500.000, steps: 500\n",
      "Episode 9: reward: -500.000, steps: 500\n",
      "Episode 10: reward: -500.000, steps: 500\n",
      "评估回合奖励: [-500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0]\n",
      "平均奖励: -500.00\n",
      "--- 评估完成 ---\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 评估代理 (里程碑2的第4步和第5步) ---\n",
    "NB_EVALUATION_EPISODES = 10 # 评估的回合数\n",
    "print(f\"\\n--- 开始评估训练后的代理，共 {NB_EVALUATION_EPISODES} 回合 ---\")\n",
    "# visualize=True 会渲染环境，这需要一个图形界面\n",
    "# 如果在没有图形界面的Docker中运行，请将 visualize 设为 False 或使用 'rgb_array' 模式并保存帧\n",
    "history = dqn.test(env, nb_episodes=NB_EVALUATION_EPISODES, visualize=False, verbose=1) # 默认为False以避免图形界面问题\n",
    "\n",
    "# 计算并记录平均奖励\n",
    "episode_rewards = history.history['episode_reward']\n",
    "average_reward = np.mean(episode_rewards)\n",
    "print(f\"评估回合奖励: {episode_rewards}\")\n",
    "print(f\"平均奖励: {average_reward:.2f}\")\n",
    "print(\"--- 评估完成 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 模型权重已保存到: dqn_acrobot_weights.h5 ---\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 保存模型权重 (里程碑2的第6步) ---\n",
    "# 只保存模型的权重，而不是整个代理对象\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "print(f\"\\n--- 模型权重已保存到: {weights_filename} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- 正在加载保存的模型权重并测试 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:48:47.196387: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_25_1/bias/Assign' id:2815 op device:{requested: '', assigned: ''} def:{{{node dense_25_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_25_1/bias, dense_25_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-06-10 14:48:47.958446: W tensorflow/c/c_api.cc:305] Operation '{name:'total_24/Assign' id:2926 op device:{requested: '', assigned: ''} def:{{{node total_24/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_24, total_24/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/local/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2025-06-10 14:48:48.147210: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_26/BiasAdd' id:2770 op device:{requested: '', assigned: ''} def:{{{node dense_26/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_26/MatMul, dense_26/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 模型权重已从 dqn_acrobot_weights.h5 加载 ---\n",
      "\n",
      "--- 开始评估加载后的代理，共 5 回合 ---\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -500.000, steps: 500\n",
      "Episode 2: reward: -500.000, steps: 500\n",
      "Episode 3: reward: -500.000, steps: 500\n",
      "Episode 4: reward: -500.000, steps: 500\n",
      "Episode 5: reward: -500.000, steps: 500\n",
      "加载后代理评估回合奖励: [-500.0, -500.0, -500.0, -500.0, -500.0]\n",
      "加载后代理平均奖励: -500.00\n",
      "--- 加载后的模型测试完成 ---\n"
     ]
    }
   ],
   "source": [
    "# --- 8. 加载和测试保存的模型 (里程碑2的第7步) ---\n",
    "print(\"\\\\n--- 正在加载保存的模型权重并测试 ---\")\n",
    "\n",
    "# 首先，创建一个新的代理实例 (或重置现有代理的状态)\n",
    "# 注意：这里我们创建一个新的代理，但其模型结构必须与保存权重时的模型结构相同\n",
    "loaded_model = build_model(input_shape, nb_actions)\n",
    "loaded_dqn = DQNAgent(\n",
    "    model=loaded_model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy, # 可以重复使用相同的策略和记忆，但对于加载测试，记忆通常不重要\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=1000,\n",
    "    gamma=.99,\n",
    "    target_model_update=10000,\n",
    "    train_interval=4,\n",
    "    processor=CustomProcessor() # <--- 在这里添加处理器\n",
    ")\n",
    "loaded_dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# 加载权重\n",
    "loaded_dqn.load_weights(weights_filename)\n",
    "print(f\"--- 模型权重已从 {weights_filename} 加载 ---\")\n",
    "\n",
    "# ... existing code ...\n",
    "# 测试加载后的代理\n",
    "NB_LOADED_TEST_EPISODES = 5 # 评估加载后的代理的回合数\n",
    "print(f\"\\n--- 开始评估加载后的代理，共 {NB_LOADED_TEST_EPISODES} 回合 ---\")\n",
    "# 同样，这里也将 visualize 设为 False\n",
    "loaded_history = loaded_dqn.test(env, nb_episodes=NB_LOADED_TEST_EPISODES, visualize=False, verbose=1)\n",
    "\n",
    "loaded_episode_rewards = loaded_history.history['episode_reward']\n",
    "loaded_average_reward = np.mean(loaded_episode_rewards)\n",
    "print(f\"加载后代理评估回合奖励: {loaded_episode_rewards}\")\n",
    "print(f\"加载后代理平均奖励: {loaded_average_reward:.2f}\")\n",
    "print(\"--- 加载后的模型测试完成 ---\")\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
